{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook to exemplify usage of InnvestigateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 3\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cpu\")\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302662\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.262101\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.244862\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.146699\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.141716\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.916380\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.749585\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.627155\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.377650\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.208793\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.089104\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.866378\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.752465\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.000873\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.796635\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.586251\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.582664\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.669054\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.551820\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.410552\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.765994\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.485778\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.408210\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.534848\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.460024\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.312507\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.445790\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.404733\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.592456\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.420107\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.334930\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.299861\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.465816\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.317004\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.476389\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.481875\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.431181\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.490746\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.423433\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.303793\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.463320\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.436727\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.417204\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.507238\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.494051\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.302837\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.218710\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.463660\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.453999\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.359028\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.378473\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.377576\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.192623\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.274483\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.612253\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.259253\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.399619\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.224376\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.375292\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.594142\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.257486\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.289840\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.246221\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.485157\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.397588\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.221999\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.461179\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.237085\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.447132\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.285954\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.321437\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.542036\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.501936\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.346829\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.309453\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.185126\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.390563\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.246795\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.209176\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.375866\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.242039\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.181272\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.213878\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.576755\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.196527\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.266227\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.182103\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.267289\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.275259\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.500642\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.214481\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.278214\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.482211\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.535392\n",
      "\n",
      "Test set: Average loss: 0.2609, Accuracy: 9221/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.252429\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.254390\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.314309\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.197398\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.330472\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.268636\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.364758\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.113459\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.298711\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.270738\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.278333\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.145051\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.106961\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.246324\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.148507\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.331202\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.143617\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.453734\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.218754\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.182248\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.217837\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.241040\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.384888\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.307571\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.154323\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.231113\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.134586\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.298313\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.260366\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.327593\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.209518\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.170491\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.333959\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.237623\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.268885\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.204534\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.126780\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.268259\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.235860\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.282847\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.121564\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.295450\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.248184\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.176017\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.247444\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.167553\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.177955\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.248853\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.133908\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.174772\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.241295\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.169984\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.149981\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.173834\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.427826\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.141961\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.206484\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.174832\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.649062\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.147390\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.154109\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.235215\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.246053\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.158334\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.176065\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.106825\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.160448\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.199820\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.265625\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.099028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.212644\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.173775\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.107351\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.160990\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.173091\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.266276\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.141964\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.103289\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.205284\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.252761\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.149075\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.126740\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.179227\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.132653\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.149927\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.151899\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.199279\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.172863\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.133491\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.361494\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.077163\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.124019\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.372510\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.179386\n",
      "\n",
      "Test set: Average loss: 0.1786, Accuracy: 9453/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.219681\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.123828\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.415734\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.272670\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.243030\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.100018\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.296445\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.249917\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.213671\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.189462\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.036186\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.080525\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.208868\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.290419\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.120982\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.164227\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.273523\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.076441\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.134288\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.073754\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.238503\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.124601\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.138430\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.170509\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.182967\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.181819\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.156536\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.223310\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.121630\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.194322\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.076470\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.133946\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.158638\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.068509\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.266561\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.163601\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.279755\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.167866\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.133758\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.141242\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.107255\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.097813\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.128767\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.233300\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.110368\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.104266\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.162010\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.208392\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.174383\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.186962\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.083630\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.150304\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.213781\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.169040\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.087099\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.114643\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.037571\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.114061\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.061864\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.079275\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.157722\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.129430\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.140734\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.181336\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.114269\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.102591\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.103150\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.156605\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.100746\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.088268\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.162733\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.075783\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.108720\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.057101\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.152021\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.098196\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.198458\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.106084\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.131885\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.127759\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.104783\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.101893\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.505167\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.084483\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.100558\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.200500\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.116577\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.223624\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.130312\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.077048\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.111648\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.117270\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.174898\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.140318\n",
      "\n",
      "Test set: Average loss: 0.1335, Accuracy: 9588/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "dropper = nn.Dropout2d()\n",
    "dropper.train(True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "torch.save(model.state_dict(), \"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.2307, -0.1654, -0.2875, -0.2318, -0.4748],\n",
       "                        [ 0.3641,  0.1395,  0.1256, -0.0614, -0.2262],\n",
       "                        [ 0.2554,  0.4101,  0.5348,  0.2844,  0.1247],\n",
       "                        [-0.0086,  0.2185,  0.3696,  0.5740,  0.4221],\n",
       "                        [-0.2582, -0.2854, -0.1485, -0.0418,  0.2308]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2094,  0.3819,  0.2198, -0.3660, -0.1862],\n",
       "                        [ 0.2370,  0.4768,  0.1684, -0.4254, -0.4751],\n",
       "                        [ 0.1823,  0.5052, -0.0108, -0.1723, -0.3510],\n",
       "                        [ 0.4847,  0.2595,  0.2439, -0.0886, -0.1668],\n",
       "                        [ 0.3812,  0.2167,  0.2505,  0.0284,  0.0534]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3928, -0.3291, -0.0130,  0.1084,  0.5241],\n",
       "                        [-0.1582, -0.2187, -0.0559,  0.4290,  0.3817],\n",
       "                        [-0.3226,  0.1402,  0.4294,  0.5776,  0.0893],\n",
       "                        [ 0.1097,  0.2760,  0.3812,  0.1309, -0.1266],\n",
       "                        [ 0.3666,  0.2354,  0.3026,  0.0853, -0.0204]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0200,  0.0184,  0.0717, -0.1704,  0.1859],\n",
       "                        [-0.1858, -0.1893, -0.0088, -0.0483,  0.1398],\n",
       "                        [-0.1508, -0.2237,  0.0854, -0.0978,  0.0746],\n",
       "                        [ 0.0219, -0.2235, -0.0758, -0.0526, -0.1904],\n",
       "                        [-0.1313, -0.0128, -0.0762, -0.1695, -0.1215]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.5347,  0.2683,  0.3473,  0.2833,  0.0453],\n",
       "                        [ 0.1496,  0.5389,  0.3609,  0.4368,  0.3017],\n",
       "                        [ 0.3013,  0.4240,  0.4758,  0.1027,  0.2188],\n",
       "                        [ 0.2612,  0.2524,  0.2372, -0.0620, -0.0720],\n",
       "                        [-0.1694, -0.0159,  0.2053, -0.1141,  0.1466]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.0853, -0.2370,  0.0664,  0.2108,  0.4229])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0475,  0.0123,  0.0425,  ...,  0.1081, -0.0762,  0.0740],\n",
       "                      [-0.0100, -0.0189, -0.0616,  ...,  0.0118, -0.0459,  0.0019],\n",
       "                      [ 0.0287, -0.0207,  0.0580,  ...,  0.0038, -0.0173, -0.0527],\n",
       "                      ...,\n",
       "                      [ 0.0211,  0.0384, -0.0504,  ...,  0.0425, -0.0603,  0.0344],\n",
       "                      [ 0.0348,  0.0113, -0.0623,  ...,  0.0733, -0.0046, -0.0604],\n",
       "                      [-0.0125, -0.0023,  0.0295,  ...,  0.0868, -0.0126, -0.0168]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0225,  0.0915,  0.0594,  0.0191, -0.0624, -0.0179, -0.0622, -0.0371,\n",
       "                      -0.0287, -0.0016,  0.0796, -0.0667,  0.0680,  0.0289, -0.0638, -0.0086,\n",
       "                      -0.0528,  0.0155,  0.0206, -0.0385,  0.0606, -0.0281, -0.0135,  0.0108,\n",
       "                      -0.0439,  0.0045, -0.0469, -0.0026,  0.0007, -0.0655,  0.0758,  0.0105,\n",
       "                       0.0556,  0.0256, -0.0179, -0.0201, -0.0061, -0.0126,  0.0237, -0.0009,\n",
       "                      -0.0392,  0.0247,  0.0859, -0.0103,  0.0370, -0.0351, -0.0564, -0.0782,\n",
       "                      -0.0229,  0.0394])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.2009,  0.0859,  0.0761, -0.1400, -0.0101, -0.0448,  0.0589, -0.0593,\n",
       "                       -0.0041, -0.2314,  0.0941,  0.0417, -0.3000,  0.0751,  0.0020, -0.1118,\n",
       "                        0.1573, -0.0737,  0.0650, -0.0258, -0.0906, -0.1153, -0.0006, -0.0546,\n",
       "                        0.3662, -0.2834, -0.1821, -0.0241, -0.1817, -0.1173, -0.2200,  0.1396,\n",
       "                       -0.0288,  0.1702,  0.1608, -0.0759,  0.1643, -0.1023,  0.2034,  0.1747,\n",
       "                        0.1113, -0.0290,  0.0900, -0.0101,  0.0738,  0.0344, -0.1824,  0.1266,\n",
       "                       -0.1953,  0.1457],\n",
       "                      [-0.2775, -0.1488,  0.2073, -0.2391, -0.0819,  0.0373, -0.2398,  0.1827,\n",
       "                        0.0410, -0.1942, -0.1232, -0.1463,  0.0224, -0.0576, -0.1029, -0.1100,\n",
       "                       -0.1394,  0.0296,  0.0226,  0.1178,  0.3537,  0.0053,  0.0589,  0.2390,\n",
       "                        0.0039,  0.1757,  0.2686, -0.1237,  0.2024,  0.1192,  0.0180,  0.0593,\n",
       "                        0.1861, -0.1634, -0.2059, -0.0252,  0.1744,  0.1096, -0.0434, -0.0069,\n",
       "                        0.0922, -0.0020,  0.0439,  0.0463, -0.0834, -0.1211, -0.1940, -0.0388,\n",
       "                        0.2555,  0.0490],\n",
       "                      [-0.1864, -0.1063, -0.0761,  0.0821, -0.0888, -0.0065,  0.1230, -0.1672,\n",
       "                        0.2330, -0.2356,  0.4020,  0.0702,  0.0936, -0.1591, -0.0782, -0.0462,\n",
       "                        0.0190,  0.1760,  0.1360,  0.1527, -0.2432,  0.2005,  0.0841, -0.3046,\n",
       "                        0.2041, -0.0335, -0.0567,  0.0318,  0.0919, -0.2893, -0.0125,  0.2871,\n",
       "                        0.1252,  0.2999, -0.2562,  0.1899, -0.0902,  0.0755,  0.0995,  0.1441,\n",
       "                        0.0384, -0.0837,  0.1096, -0.1214, -0.0038, -0.1867,  0.0091, -0.0368,\n",
       "                        0.0579,  0.1445],\n",
       "                      [-0.1398,  0.2602,  0.0261,  0.1277,  0.1051, -0.1228, -0.1254, -0.0154,\n",
       "                        0.1520,  0.2047,  0.0193,  0.1939, -0.1591, -0.1327, -0.3269, -0.0006,\n",
       "                        0.1217, -0.0060, -0.0741,  0.0376, -0.0891,  0.2001, -0.0567, -0.1796,\n",
       "                       -0.0809, -0.0221, -0.0911, -0.0364,  0.3220, -0.1630,  0.3642,  0.0449,\n",
       "                       -0.0478, -0.1322,  0.2140, -0.0939, -0.0326, -0.0370, -0.2273,  0.1143,\n",
       "                       -0.1276,  0.0730, -0.1722,  0.1392,  0.2705, -0.0179, -0.0731,  0.0397,\n",
       "                       -0.0461,  0.1159],\n",
       "                      [ 0.2951, -0.0786,  0.0702,  0.1799,  0.0046,  0.1670,  0.3091,  0.0257,\n",
       "                       -0.1142, -0.2488, -0.0375, -0.1514,  0.2310, -0.0347,  0.2815, -0.0775,\n",
       "                       -0.0573,  0.0671, -0.0271,  0.0605,  0.0902,  0.0536, -0.0583,  0.1291,\n",
       "                       -0.3887, -0.2165,  0.1683, -0.0811,  0.2085,  0.0751, -0.1419,  0.0625,\n",
       "                       -0.2506, -0.2594,  0.0990, -0.0909, -0.2679, -0.1133,  0.2103,  0.0758,\n",
       "                        0.1775, -0.0719, -0.2216, -0.0547, -0.0169, -0.1531,  0.2821,  0.0035,\n",
       "                        0.0765,  0.0286],\n",
       "                      [ 0.0337,  0.0303, -0.1506, -0.2239,  0.0844,  0.0992,  0.0250,  0.2684,\n",
       "                       -0.0074,  0.2212, -0.1582,  0.0182, -0.0028, -0.1509,  0.1406, -0.0227,\n",
       "                        0.0957,  0.0464, -0.0130,  0.2661, -0.0686,  0.1377, -0.1163,  0.2467,\n",
       "                       -0.0214,  0.2155, -0.3823, -0.0336, -0.3846, -0.0234,  0.2840,  0.0070,\n",
       "                       -0.3602,  0.1054,  0.2113,  0.1865, -0.1567,  0.0560,  0.0054, -0.1179,\n",
       "                       -0.0807,  0.0916,  0.0130, -0.0604,  0.2213,  0.3728,  0.0080, -0.0348,\n",
       "                        0.2307, -0.0895],\n",
       "                      [ 0.1891, -0.3364, -0.0166, -0.3452, -0.0910, -0.0481,  0.2332,  0.2000,\n",
       "                       -0.0762,  0.0618,  0.0092, -0.0083, -0.1489, -0.0911,  0.2055,  0.0300,\n",
       "                        0.0552, -0.0162,  0.0740, -0.1235,  0.0246, -0.1111, -0.1268,  0.0015,\n",
       "                       -0.0738, -0.0836, -0.1014, -0.0273, -0.0865, -0.0327, -0.3793,  0.2309,\n",
       "                        0.2515, -0.2603,  0.0457, -0.0960,  0.1211,  0.1989,  0.0889, -0.0786,\n",
       "                       -0.1869, -0.0138,  0.2519, -0.0289,  0.2333, -0.0241,  0.1239,  0.0478,\n",
       "                       -0.1465,  0.0536],\n",
       "                      [-0.1239,  0.3083, -0.0599,  0.0796,  0.1058,  0.2863, -0.0702, -0.0402,\n",
       "                       -0.0552, -0.1150,  0.2891, -0.0170,  0.3764, -0.0183, -0.0802,  0.0769,\n",
       "                        0.1035,  0.0216,  0.0256, -0.0040,  0.2490,  0.3254, -0.0381,  0.1917,\n",
       "                        0.1714, -0.3823, -0.1592,  0.0178,  0.0115,  0.1215, -0.0894, -0.0576,\n",
       "                       -0.1303, -0.1542,  0.2304,  0.0916,  0.2247,  0.2419,  0.1402, -0.2081,\n",
       "                        0.1907,  0.0979, -0.0191,  0.0329, -0.4167, -0.1023, -0.1539,  0.1259,\n",
       "                        0.1174, -0.0126],\n",
       "                      [ 0.1610, -0.0768, -0.2022, -0.0154,  0.0570,  0.1317,  0.0297, -0.2195,\n",
       "                        0.0266,  0.0818, -0.1284,  0.0911, -0.0542, -0.1815,  0.1789,  0.1249,\n",
       "                        0.1271, -0.0489, -0.0709, -0.1447, -0.2116, -0.2789,  0.0046, -0.1590,\n",
       "                       -0.1182,  0.1174, -0.0537,  0.0562,  0.0897, -0.0011,  0.1692, -0.2570,\n",
       "                        0.2241,  0.3294, -0.0063, -0.0409,  0.0100, -0.0070, -0.1001, -0.3171,\n",
       "                        0.1470, -0.0791, -0.0215, -0.0220,  0.0485, -0.0302, -0.0957,  0.0844,\n",
       "                       -0.2233, -0.1854],\n",
       "                      [ 0.1200,  0.3692, -0.0036,  0.4312, -0.0869,  0.1333, -0.0612, -0.1089,\n",
       "                       -0.1929,  0.1391, -0.2270,  0.1205, -0.1888,  0.0296,  0.1671,  0.1488,\n",
       "                        0.0012, -0.1223,  0.1462,  0.1419,  0.1051, -0.1585, -0.0546,  0.1472,\n",
       "                        0.0620, -0.1343,  0.1793, -0.0131, -0.0516, -0.0191, -0.0983, -0.3723,\n",
       "                       -0.2596, -0.1345, -0.1589, -0.1597,  0.0367, -0.2400, -0.1872, -0.0436,\n",
       "                       -0.0405,  0.1023,  0.1362,  0.1372, -0.0059,  0.1787,  0.1340,  0.0309,\n",
       "                        0.0763,  0.0011]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-7.1267e-02,  3.3917e-02,  1.5026e-01,  1.2157e-02, -5.1224e-02,\n",
       "                       6.2476e-02, -1.3044e-01, -9.6570e-03, -1.7670e-01, -1.0274e-04]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innvestigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"./model\"))\n",
    "# Convert to innvestigate model\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5, res_rule=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: LogSoftmax layer was turned into probabilities.\n",
      " \n",
      "\n",
      "layer: LogSoftmax() - relevance: torch.Size([20, 10])\n",
      "\n",
      "layer: Linear(in_features=50, out_features=10, bias=True) - relevance: torch.Size([20, 50])\n",
      "\n",
      "layer: ReLU() - relevance: torch.Size([20, 50])\n",
      "\n",
      "layer: Linear(in_features=180, out_features=50, bias=True) - relevance: torch.Size([20, 180])\n",
      "\n",
      "layer: ReLU() - relevance: torch.Size([20, 180])\n",
      "\n",
      "layer: MaxPool2d(kernel_size=2, stride=(2, 2), padding=0, dilation=1, ceil_mode=False) - relevance: torch.Size([20, 5, 12, 12])\n",
      "\n",
      "torch.Size([20, 1, 27, 27]) torch.Size([20, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (27) must match the size of tensor b (28) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-62f3e90573cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Overlay with noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# data[0] += 0.25 * data[0].max() * torch.Tensor(np.random.randn(28*28).reshape(1, 28, 28))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_relevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minnvestigate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ML_MIP/ML_MIP/Pytorch-LRP/innvestigator.py\u001b[0m in \u001b[0;36minnvestigate\u001b[0;34m(self, in_tensor, rel_for_class)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m#print(f'layer: {layer}\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# Compute layer specific backwards-propagation of relevance values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mrelevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_propagated_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'layer: {layer} - relevance: {relevance.shape}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ML_MIP/ML_MIP/Pytorch-LRP/inverter_util.py\u001b[0m in \u001b[0;36mcompute_propagated_relevance\u001b[0;34m(self, layer, relevance)\u001b[0m\n\u001b[1;32m     94\u001b[0m         elif isinstance(layer,\n\u001b[1;32m     95\u001b[0m                       (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d)):\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_nd_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ML_MIP/ML_MIP/Pytorch-LRP/inverter_util.py\u001b[0m in \u001b[0;36mconv_nd_inverse\u001b[0;34m(self, m, relevance_in)\u001b[0m\n\u001b[1;32m    450\u001b[0m                                             groups=m.groups)\n\u001b[1;32m    451\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevance_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mrelevance_out\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (27) must match the size of tensor b (28) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "for data, target in test_loader:\n",
    "\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    batch_size = int(data.size()[0])\n",
    "\n",
    "    evidence_for_class = []\n",
    "    # Overlay with noise \n",
    "    # data[0] += 0.25 * data[0].max() * torch.Tensor(np.random.randn(28*28).reshape(1, 28, 28))\n",
    "    model_prediction, true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "\n",
    "    for i in range(10):\n",
    "        # Unfortunately, we had some issue with freeing pytorch memory, therefore \n",
    "        # we need to reevaluate the model separately for every class.\n",
    "        model_prediction, input_relevance_values = inn_model.innvestigate(in_tensor=data, rel_for_class=i)\n",
    "        evidence_for_class.append(input_relevance_values)\n",
    "    \n",
    "    \n",
    "    evidence_for_class = np.array([elt.numpy() for elt in evidence_for_class])\n",
    "    for idx, example in enumerate(data):\n",
    "\n",
    "        prediction = np.argmax(model_prediction.detach(), axis=1)\n",
    "        \n",
    "\n",
    "        fig, axes = plt.subplots(3, 5)\n",
    "        fig.suptitle(\"Prediction of model: \" + str(prediction[idx]) + \"({0:.2f})\".format(\n",
    "            100*float(model_prediction[idx][model_prediction[idx].argmax()].exp()/model_prediction[idx].exp().sum())))\n",
    "        \n",
    "        vmin = np.percentile(evidence_for_class[:, idx], 50)\n",
    "        vmax = np.percentile(evidence_for_class[:, idx], 99.9)\n",
    "        \n",
    "        axes[0, 2].imshow(example[0])\n",
    "        axes[0, 2].set_title(\"Input (\" + str(int(target[idx]))+ \")\")\n",
    "        axes[0, 3].imshow(evidence_for_class[prediction[idx]][idx][0], vmin=vmin,\n",
    "                          vmax=vmax, cmap=\"hot\")\n",
    "        axes[0, 3].set_title(\"Pred. Evd.\")\n",
    "        for ax in axes[0]:\n",
    "            ax.set_axis_off()\n",
    "        for j, ax in enumerate(axes[1:].flatten()):\n",
    "            im = ax.imshow(evidence_for_class[j][idx][0], cmap=\"hot\", vmin=vmin,\n",
    "                          vmax=vmax)\n",
    "            ax.set_axis_off()\n",
    "            ax.set_title(\"Evd. \" + str(j))\n",
    "        fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "        plt.show()\n",
    "    break\n",
    "        # fig.savefig(\"./mnist_example{0}.png\".format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
