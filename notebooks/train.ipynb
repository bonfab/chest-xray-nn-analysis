{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe56dd4328f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNaiveResizeGray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResizeCropGray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBinaryClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import src_loader\n",
    "from datasets import NaiveResizeGray, ResizeCropGray, BinaryClassification\n",
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "    \n",
    "TWO_CLASS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_y, max_y, min_x, max_x = datasets.determine_sizes('Data/CheXpert-v1.0', ['Data/CheXpert-v1.0/train-45000-reduced.csv', 'Data/CheXpert-v1.0/valid-5000-reduced.csv'])\n",
    "#center_crop_size = min(min_y, min_x)\n",
    "#print(min_y, max_y, min_x, max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = BinaryClassification('Data/CheXpert-v1.0/train-45000-reduced.csv')#, mean = 0.00010563735961914062, std=0.010277460797675398)\n",
    "#print(training_set.mean, training_set.std)\n",
    "validation_set = BinaryClassification('Data/CheXpert-v1.0/valid-5000-reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_OF_WORKERS = 20\n",
    "\n",
    "VAL_BATCH_SIZE = 16\n",
    "VAL_NUM_WORKERS = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_OF_WORKERS, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(validation_set, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=VAL_NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testNet_2class(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(12, 12), stride=(6, 6))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1600, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%aimport models\n",
    "%aimport datasets\n",
    "#%env CUDA_LAUNCH_BLOCKING=1\n",
    "#model = models.SimpleNet(training_set.num_pathologies, two_class=TWO_CLASS).to(device)\n",
    "model = models.testNet_2class(training_set.num_pathologies, training_set.output_size).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_per_feature = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_3class(y_hat, y):\n",
    "    \n",
    "    batch_loss = loss_per_feature(y_hat[:,0:2], y[:,0])\n",
    "    for i in range(int((y_hat.shape[1]-2)/3)):\n",
    "        batch_loss += loss_per_feature(y_hat[:,(i*3+2):((i+1)*3+2)], y[:,i+1])\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = iter(val_loader)\n",
    "\n",
    "def validate_batch():\n",
    "    with torch.no_grad():\n",
    "        global val_iter\n",
    "        try:\n",
    "            val_x, val_y = next(val_iter)\n",
    "        except StopIteration:\n",
    "            val_iter = iter(val_loader)\n",
    "            val_x, val_y = next(val_iter)\n",
    "            \n",
    "        val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "        y_hat = model(val_x)\n",
    "        \n",
    "        return loss(y_hat, val_y)\n",
    "    \n",
    "def validate_batch_2class():\n",
    "    with torch.no_grad():\n",
    "        global val_iter\n",
    "        try:\n",
    "            val_x, val_y = next(val_iter)\n",
    "        except StopIteration:\n",
    "            val_iter = iter(val_loader)\n",
    "            val_x, val_y = next(val_iter)\n",
    "            \n",
    "        val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "        y_hat = model(val_x)\n",
    "        \n",
    "        return criterion(y_hat, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_validation_loss_3class():\n",
    "    with torch.set_grad_enabled(False):\n",
    "        val_loss = 0\n",
    "        for val_x, val_y in val_loader:\n",
    "            x, y = val_x.to(device), val_y.to(device)\n",
    "            \n",
    "            val_loss += loss(model(x), y)\n",
    "        empirical_loss = val_loss.item()/validation_set.__len__()\n",
    "        print('Empirical Validation Loss: {}'.format(empirical_loss))\n",
    "    return empirical_loss\n",
    "\n",
    "def total_validation_loss_2class():\n",
    "    with torch.set_grad_enabled(False):\n",
    "        val_loss = 0\n",
    "        for val_x, val_y in val_loader:\n",
    "            x, y = val_x.to(device), val_y.to(device)\n",
    "            \n",
    "            val_loss += criterion(model(x), y)\n",
    "        empirical_loss = val_loss.item()/validation_set.__len__()\n",
    "        print('Empirical Validation Loss: {}'.format(empirical_loss))\n",
    "    return empirical_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Empirical Validation Loss: 0.04225258255004883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9c9b111c504bd0898dadeb207c520d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1438.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Empirical Validation Loss: 0.03290546417236328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94fd485ed004eed89b5908ca27d047a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1438.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Empirical Validation Loss: 0.03351863861083984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e2dbf487af427297752ff5aa21ca87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1438.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Empirical Validation Loss: 0.032892040252685543\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "print(device)\n",
    "\n",
    "total_validation_loss_2class()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    with tqdm(train_loader) as pbar:\n",
    "        for i, batch in enumerate(pbar):\n",
    "            \n",
    "            x, y = batch\n",
    "            #print(x[0])\n",
    "            #break\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                val_loss = validate_batch_2class().item()/VAL_BATCH_SIZE\n",
    "            \n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            #print(y.dtype)\n",
    "            #print(y_hat.dtype)\n",
    "            batch_loss = criterion(y_hat, y)\n",
    "            \n",
    "            #print(y_hat, y)\n",
    "            \n",
    "            pbar.set_postfix(loss=batch_loss.item()/BATCH_SIZE, val_loss=val_loss)\n",
    "            \n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    total_validation_loss_2class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'trained/NaiveResizeGray/first_reference.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
